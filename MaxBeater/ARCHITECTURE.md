# MaxBeater Architecture Deep Dive

## ğŸ›ï¸ System Architecture

### High-Level Design Philosophy

MaxBeater follows a **modular, layered architecture** with strict separation of concerns:

1. **Perception Layer**: Trapdoor belief tracking
2. **Evaluation Layer**: Heuristic + learned value estimation
3. **Search Layer**: MCTS with fallback strategies
4. **Orchestration Layer**: Time management and decision routing

---

## ğŸ”„ Data Flow

```
Game Engine
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PlayerAgent.play(board, sensors, time)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. TrapdoorBelief.update(sensors)       â”‚
â”‚    â†’ Bayesian belief update             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Time Budget Calculation              â”‚
â”‚    â†’ Decide: MCTS vs Fallback           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   MCTS Search        â”‚  Fallback Search â”‚
â”‚   (if time > 15s)    â”‚  (if time < 15s) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“                         â†“
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ For each candidate position:            â”‚
â”‚   Evaluator.evaluate(board)             â”‚
â”‚   â”œâ”€ Heuristic (always)                 â”‚
â”‚   â””â”€ ValueModel (optional)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Return best move (Direction, MoveType)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Game Engine applies move
```

---

## ğŸ§© Component Interactions

### Component Dependency Graph

```
PlayerAgent (agent.py)
    â”‚
    â”œâ”€â†’ TrapdoorBelief (belief.py)
    â”‚       â””â”€â†’ game.Board
    â”‚
    â”œâ”€â†’ Evaluator (evaluator.py)
    â”‚       â”œâ”€â†’ TrapdoorBelief
    â”‚       â”œâ”€â†’ Features (features.py)
    â”‚       â””â”€â†’ ValueModelRuntime (value_model_runtime.py)
    â”‚               â””â”€â†’ value_weights.npz (optional)
    â”‚
    â”œâ”€â†’ MCTSSearch (search_mcts.py)
    â”‚       â”œâ”€â†’ Evaluator
    â”‚       â”œâ”€â†’ TrapdoorBelief
    â”‚       â””â”€â†’ game.Board (forecast_move, reverse_perspective)
    â”‚
    â””â”€â†’ FallbackSearch (search_fallback.py)
            â”œâ”€â†’ Evaluator
            â”œâ”€â†’ TrapdoorBelief
            â””â”€â†’ game.Board
```

**Key Design Decisions**:
- âœ… Shared `TrapdoorBelief` instance across all components
- âœ… `Evaluator` encapsulates both heuristic and ML models
- âœ… Search engines only depend on `Evaluator` interface (clean abstraction)

---

## ğŸ” Detailed Component Specs

### 1. TrapdoorBelief (belief.py)

**State**:
```python
belief_even: Dict[Coord, float]  # P(trap at coord) for even-parity trap
belief_odd: Dict[Coord, float]   # P(trap at coord) for odd-parity trap
```

**Update Algorithm**:
```python
def update(board, sensor_data):
    for each trapdoor i:
        heard_i, felt_i = sensor_data[i]
        for each position (x, y):
            dx, dy = distance(player_pos, (x, y))
            likelihood = P(heard_i, felt_i | trap at (x,y))
            belief[i][(x, y)] *= likelihood
        normalize(belief[i])
```

**Complexity**:
- Time: O(NÂ²) per update (N=8, so 64 cells)
- Space: O(NÂ²) for belief grids

---

### 2. Features (features.py)

**Scalar Features (26-dimensional)**:
```python
[
    egg_diff,              # 0: Main objective
    my_eggs, enemy_eggs,   # 1-2: Raw counts
    my_mobility, enemy_mobility, mobility_diff,  # 3-5
    turns_left_me, turns_left_enemy, turn_count, turn_progress,  # 6-9
    my_turds, enemy_turds, turd_diff,  # 10-12
    my_dist_center, enemy_dist_center,  # 13-14
    chicken_dist,  # 15
    my_corner_dist, enemy_corner_dist,  # 16-17
    my_on_corner, enemy_on_corner,  # 18-19
    can_lay_egg,  # 20
    my_trap_risk, enemy_trap_risk, my_max_risk_nearby  # 21-23
    # ... (26 total)
]
```

**Board Tensor (14Ã—8Ã—8)**:
```python
Channel 0:  My position (one-hot)
Channel 1:  Enemy position (one-hot)
Channel 2:  My eggs (binary)
Channel 3:  Enemy eggs (binary)
Channel 4:  My turds (binary)
Channel 5:  Enemy turds (binary)
Channel 6:  Trapdoor belief (probability)
Channel 7:  Found trapdoors (binary)
Channel 8:  My valid egg squares (parity)
Channel 9:  Enemy valid egg squares (parity)
Channel 10: Corners (static)
Channel 11: Distance to center (gradient)
Channel 12: Turn progress (broadcast)
Channel 13: Egg differential (broadcast, normalized)
```

---

### 3. ValueModelRuntime (value_model_runtime.py)

**Network Architecture**:
```
Input Layer:     1050 units (14*8*8 + 26)
    â†“ [W1: 256Ã—1050, b1: 256]
Hidden Layer 1:  256 units + ReLU
    â†“ [W2: 128Ã—256, b2: 128]
Hidden Layer 2:  128 units + ReLU
    â†“ [W3: 1Ã—128, b3: 1]
Output Layer:    1 unit + tanh â†’ [-1, 1]
```

**Forward Pass** (NumPy only):
```python
def forward(x):
    h1 = relu(W1 @ x + b1)      # (256,)
    h2 = relu(W2 @ h1 + b2)     # (128,)
    out = tanh(W3 @ h2 + b3)    # (1,)
    return out[0]
```

**Weight Format** (`value_weights.npz`):
```python
{
    'W1': (256, 1050),  # Hidden1 weights
    'b1': (256,),       # Hidden1 bias
    'W2': (128, 256),   # Hidden2 weights
    'b2': (128,),       # Hidden2 bias
    'W3': (1, 128),     # Output weights
    'b3': (1,),         # Output bias
}
```

---

### 4. Evaluator (evaluator.py)

**Heuristic Function**:
```python
score = 0

# Primary: Egg differential (1000Ã—)
egg_weight = 1000 * (1.5 if endgame else 1.0)
score += (my_eggs - enemy_eggs) * egg_weight

# Mobility
if my_moves == 0:
    score -= 500  # Blocked penalty
else:
    score += my_moves * 15

if enemy_moves == 0:
    score += 500  # Blocking bonus
else:
    score -= enemy_moves * 15

# Corners (3Ã— egg value)
if on_corner and can_lay_egg:
    score += 200

# Trapdoor risk
score -= my_trap_risk * 150
score -= my_nearby_risk * 50

# Turds
score += (my_turds - enemy_turds) * 30

# Center control (midgame)
if turns_left > 15:
    score += (enemy_center_dist - my_center_dist) * 10

return score
```

**Value Model Blending**:
```python
h = heuristic(board)
v = value_model.forward(features) * 1000  # Scale to heuristic range

if abs(h) > 2000:
    return h  # Trust heuristic for extreme positions
elif abs(h) > 1000:
    return 0.7*h + 0.3*v  # Blend 70-30
else:
    return 0.5*h + 0.5*v  # Blend 50-50
```

---

### 5. MCTSSearch (search_mcts.py)

**UCT Formula**:
```python
def ucb_score(node, c_puct=1.5):
    if node.N == 0:
        # Unvisited: infinite exploration
        return Q + c_puct * P * sqrt(parent.N + 1)
    else:
        exploration = c_puct * P * sqrt(parent.N) / (1 + node.N)
        return Q + exploration
```

**MCTS Loop**:
```python
for sim in range(max_simulations):
    # 1. Selection: Follow UCT down to leaf
    node = root
    path = [node]
    while not node.is_leaf():
        node = select_best_child_uct(node)
        path.append(node)
    
    # 2. Expansion: Create children
    if not node.is_terminal:
        expand_node(node)
        if node.children:
            node = select_child(node)
            path.append(node)
    
    # 3. Evaluation: Use value model (no rollout)
    value = evaluator.evaluate(node.board)
    value = tanh(value / 2000)  # Normalize to [-1, 1]
    
    # 4. Backpropagation: Update all ancestors
    for n in reversed(path):
        n.N += 1
        n.W += value
        n.Q = n.W / n.N
        value = -value  # Flip for opponent
```

**Time Budget**:
```python
time_budget = min(
    (remaining_time - safety_margin) / turns_left,
    10.0  # Max 10s per move
)
```

---

### 6. FallbackSearch (search_fallback.py)

**One-Ply Greedy**:
```python
for each move in valid_moves:
    child_board = apply_move(board, move)
    score = -evaluator.quick_evaluate(child_board)  # Negated (opponent's view)
    if score > best_score:
        best_score = score
        best_move = move
return best_move
```

**Two-Ply Minimax**:
```python
for each our_move in valid_moves:
    after_our_move = apply_move(board, our_move)
    
    worst_enemy_score = +inf
    for each enemy_move in enemy_valid_moves:
        after_enemy_move = apply_move(after_our_move, enemy_move)
        enemy_score = -evaluator.quick_evaluate(after_enemy_move)
        worst_enemy_score = min(worst_enemy_score, enemy_score)
    
    if worst_enemy_score > best_score:
        best_score = worst_enemy_score
        best_move = our_move
return best_move
```

---

## ğŸ”„ Perspective Handling (CRITICAL)

### The Challenge

The game engine alternates turns:
- `board.chicken_player` = current player (whose turn it is)
- `board.chicken_enemy` = waiting player

After a move, perspectives must flip for adversarial search.

### Our Solution

**Every time we apply a move**:
```python
new_board = board.forecast_move(direction, move_type)
new_board.reverse_perspective()  # CRITICAL!
```

**After `reverse_perspective()`**:
- `new_board.chicken_player` = opponent (who moves next)
- `new_board.chicken_enemy` = us (who just moved)

**In MCTS backpropagation**:
```python
for node in reversed(path):
    node.W += value
    value = -value  # Flip sign at each level
```

**Why this works**:
- Each level of the tree represents alternating players
- Value sign flip implements minimax correctly
- Evaluator always sees "current player" in `chicken_player`

---

## âš¡ Performance Characteristics

### Time Complexity

| Component | Per Call | Notes |
|-----------|----------|-------|
| TrapdoorBelief.update | O(NÂ²) | N=8, fast |
| Feature extraction | O(NÂ²) | N=8, fast |
| ValueModel.forward | O(1050Ã—256 + 256Ã—128 + 128) | ~0.3ms |
| Heuristic eval | O(M) | M=# valid moves, ~12 |
| MCTS simulation | O(DÃ—MÃ—E) | D=depth, M=moves, E=eval |
| Full MCTS | O(SÃ—DÃ—MÃ—E) | S=simulations, typically 500-2000 |

**Typical Turn Time**: 1-3 seconds (1000 MCTS simulations)

### Space Complexity

| Component | Memory |
|-----------|--------|
| TrapdoorBelief | O(NÂ²) | ~1KB |
| ValueModel weights | ~550KB | W1:256Ã—1050, W2:128Ã—256, W3:1Ã—128 |
| MCTS tree | O(SÃ—M) | ~10MB for 2000 sims |
| Feature tensors | ~5KB per state | Negligible |

**Total Memory**: ~50MB peak

---

## ğŸ¯ Design Trade-offs

### MCTS vs Alpha-Beta

**We chose MCTS because**:
- âœ… Better for positions with many valid moves (12-16 typical)
- âœ… Anytime algorithm (can stop early if time runs out)
- âœ… Naturally handles stochastic evaluation (value model)
- âœ… Explores promising lines deeply without pruning risks

**Alpha-Beta would be better for**:
- âš ï¸ Positions with few moves (2-4)
- âš ï¸ Perfect evaluation functions
- âš ï¸ Very deep tactical lines

**Our hybrid approach**: Use MCTS normally, fallback to 2-ply alpha-beta when time is low

---

### Heuristic vs Pure Learned

**We use hybrid (heuristic + value model) because**:
- âœ… Heuristic provides strong baseline immediately
- âœ… Value model adds nuanced understanding (when trained)
- âœ… Graceful degradation if weights unavailable
- âœ… Blending prevents ML model from catastrophic failures

**Pure learned would**:
- âš ï¸ Require extensive training
- âš ï¸ Risk overfitting to training opponents
- âš ï¸ Lack interpretability

---

## ğŸ”’ Safety & Robustness

### Multiple Fallback Layers

1. **MCTS fails** â†’ Use FallbackSearch
2. **FallbackSearch fails** â†’ Use one-ply greedy
3. **Move invalid** â†’ Choose random valid move
4. **No valid moves** â†’ Return default (engine will reject gracefully)

### Time Management

```python
time_safety_margin = 3.0  # Never use last 3 seconds
min_time_per_move = 0.2   # Always think at least 0.2s

# Check time frequently during search
if time_left() < safety_margin:
    abort_search()
```

### Numerical Stability

- All probabilities use epsilon (0.01) instead of 0
- Value model outputs tanh-bounded [-1, 1]
- Heuristic scores clipped when blending
- Beliefs normalized after each update

---

## ğŸ“Š Benchmarking & Profiling

### Typical Breakdown (1 turn)

```
Belief update:      10ms   (1%)
Feature extraction: 5ms    (0.5%)
MCTS search:       1500ms  (95%)
  - Node selection: 300ms  (20%)
  - Expansion:      200ms  (13%)
  - Evaluation:     800ms  (53%)
    - Heuristic:    400ms
    - Value model:  400ms
  - Backprop:       200ms  (13%)
Safety checks:      10ms   (1%)
Total:            ~1525ms
```

### Optimization Opportunities

1. **Value model vectorization**: Batch evaluations (10-20% speedup)
2. **Heuristic caching**: Cache common board patterns (5-10% speedup)
3. **Move ordering**: Better priors for MCTS (15-25% fewer simulations)
4. **Compiled NumPy**: Use `numba` JIT (2-3Ã— speedup, but adds dependency)

---

## ğŸ§ª Testing Strategy

### Unit Tests (Recommended)

```python
# test_belief.py
def test_belief_initialization():
    belief = TrapdoorBelief(8)
    assert sum(belief.belief_even.values()) â‰ˆ 1.0

def test_belief_update():
    belief = TrapdoorBelief(8)
    # ... create mock board, sensors
    belief.update(board, [(True, False), (False, False)])
    # Assert beliefs changed appropriately

# test_evaluator.py
def test_heuristic_egg_diff():
    # Board with +5 egg advantage should score +5000
    score = evaluator.heuristic(board)
    assert score > 4000

# test_mcts.py
def test_mcts_returns_valid_move():
    move = mcts.choose_move(board, time_left, turns_left)
    assert move in board.get_valid_moves()
```

### Integration Tests

```python
def test_full_game():
    agent = PlayerAgent(board, time_left)
    for turn in range(40):
        move = agent.play(board, sensors, time_left)
        assert move in board.get_valid_moves()
        board.apply_move(move[0], move[1])
```

---

## ğŸ“ Key Learnings & Best Practices

### 1. Perspective Management is CRITICAL
- Always reverse after applying opponent moves
- Double-check value sign flips in minimax
- Test with simple 2-ply scenarios first

### 2. Time Management is Essential
- Reserve safety margin (3-5s)
- Use adaptive budgets based on turns left
- Have fast fallbacks ready

### 3. Feature Engineering Matters
- Egg differential dominates (weight 1000Ã—)
- Blocking/mobility crucial (weight 500Ã—)
- Corners underestimated by naive agents (weight 200Ã—)

### 4. MCTS Hyperparameters
- `c_puct = 1.5` works well (1.0-2.0 range is robust)
- More simulations always helps, but diminishing returns >1000
- Uniform priors acceptable when policy network unavailable

### 5. Graceful Degradation
- Agent should work without value model
- Agent should work under time pressure
- Agent should never crash or return invalid moves

---

## ğŸ“– Further Reading

- See `README.md` for user documentation
- See `QUICK_START.md` for setup instructions
- See inline code comments for implementation details
- See `train_value_model.py` for ML training pipeline

---

**End of Architecture Document**


