# AlphaChicken - Deep Reinforcement Learning Agent

An **AlphaZero-style** agent for the Chicken game, combining deep neural networks with Monte Carlo Tree Search (MCTS).

## ğŸ—ï¸ Architecture

### ğŸ“‚ Files

```
AlphaChicken/
â”œâ”€â”€ model.py         - Neural network architecture (ResNet + Policy/Value heads)
â”œâ”€â”€ mcts.py          - Monte Carlo Tree Search with UCT
â”œâ”€â”€ agent.py         - Main agent interface (game engine entry point)
â”œâ”€â”€ train_pace.py    - Self-play training loop
â””â”€â”€ best_model.pt    - Trained model weights (generated by training)
```

---

## ğŸ§  Neural Network (`model.py`)

### AlphaChickenNet Architecture

```
Input (8Ã—8Ã—14 board representation)
    â†“
[Conv Block] â†’ [4Ã— Residual Blocks]
    â†“                    â†“
[Policy Head]      [Value Head]
    â†“                    â†“
12-dim vector      Scalar [-1, 1]
(move probs)       (win prob)
```

### Input Channels (14 total)

| Channel | Description | Type |
|---------|-------------|------|
| 0 | My chicken position | One-hot (8Ã—8) |
| 1 | Enemy chicken position | One-hot (8Ã—8) |
| 2 | My eggs | Binary map |
| 3 | Enemy eggs | Binary map |
| 4 | My turds | Binary map |
| 5 | Enemy turds | Binary map |
| 6-7 | Trapdoor belief maps | Probability grids |
| 8 | Valid egg positions | Binary (parity) |
| 9 | Turns left | Normalized plane |
| 10 | My turds left | Normalized plane |
| 11 | Enemy turds left | Normalized plane |
| 12 | Corner locations | Static binary |
| 13 | Distance to center | Normalized gradient |

### Output Heads

1. **Policy Head**: 12-dimensional log-softmax
   - Maps to 4 directions Ã— 3 move types
   - Mapping: `index = direction * 3 + move_type`
   
2. **Value Head**: Scalar in [-1, 1]
   - +1 = current player wins
   - -1 = current player loses
   - 0 = draw

---

## ğŸŒ² MCTS (`mcts.py`)

### Algorithm: UCT (Upper Confidence Bound for Trees)

```python
# Selection
score = Q + c_puct * P * sqrt(N_parent) / (1 + N)

# Expansion
policy_probs, value = model(state)

# Backpropagation
# Value flips sign at each level (minimax)
```

### Key Features

- **Neural network guidance**: Policy priors from network
- **Perspective flipping**: Alternates player perspective at each level
- **Valid move masking**: Only expands legal moves
- **Temperature control**: Exploration (Ï„=1) â†’ Exploitation (Ï„=0)

---

## ğŸ® Agent Interface (`agent.py`)

### Entry Point for Game Engine

```python
class PlayerAgent:
    def __init__(self, board, time_left, seed=None):
        # Load trained model (best_model.pt)
        # Initialize MCTS and TrapdoorBelief
        
    def play(self, board, sensor_data, time_left):
        # 1. Update trapdoor beliefs
        # 2. Calculate time budget
        # 3. Run MCTS simulations
        # 4. Return best move
```

### Time Management

- Allocates time proportional to remaining turns
- Safety margin of 2 seconds
- Max 5 seconds per move, min 0.1 seconds
- Runs MCTS in batches of 10 simulations

### Fallback Logic

1. If no time â†’ return first valid move
2. If move invalid â†’ choose from valid moves
3. If no children in tree â†’ random valid move

---

## ğŸ‹ï¸ Training (`train_pace.py`)

### Self-Play Training Loop

```
FOR each epoch:
    1. Generate N self-play games
    2. Store (state, policy_target, value_target) tuples
    3. Train network on replay buffer
    4. Save checkpoint
```

### Self-Play Process

1. **MCTS simulation**: 50 simulations per move
2. **Policy target**: Visit count distribution
3. **Move selection**:
   - Early game (moves 0-30): Stochastic (temperature=1)
   - Late game (moves 30+): Deterministic (temperature=0)
4. **Value target**: Assigned at game end
   - Winner: +1
   - Loser: -1
   - Draw: 0

### Loss Function

```python
loss = policy_loss + value_loss
     = -sum(policy_target * log_policy_pred) + MSE(value_pred, value_target)
```

### Hyperparameters

```python
num_epochs = 100
games_per_epoch = 50
num_simulations = 50
batch_size = 256
learning_rate = 0.001
replay_buffer_size = 10000
```

---

## ğŸš€ Usage

### Running the Agent (Competition)

The agent automatically loads `best_model.pt` if available:

```bash
# Agent will be called by game engine
# No manual invocation needed
```

### Training the Model

```bash
cd 3600-agents/AlphaChicken
python train_pace.py
```

**Note**: Training is computationally intensive. Recommended:
- GPU acceleration (CUDA)
- 8+ CPU cores for parallel self-play
- 8-24 hours for initial training

### Output Files

- `checkpoints/model_epoch_N.pt` - Checkpoint after each epoch
- `best_model.pt` - Latest trained model (used by agent)

---

## ğŸ“Š Training Progress

Monitor training with:

```
Epoch 1/100
  Game 10/50 complete, buffer size: 1234
  ...
Epoch Summary:
  Policy Loss: 2.1234
  Value Loss:  0.5678
  Total Loss:  2.6912
  Time: 120.5s
```

---

## ğŸ”§ Technical Details

### Trapdoor Belief Tracking

- **Bayesian updates** based on sensor data (heard/felt)
- Separate grids for even/odd parity trapdoors
- Likelihood calculation uses game's probability functions
- Beliefs normalized after each update

### Perspective Handling

MCTS alternates perspective at each level:
1. Root node: Current player's view
2. Child nodes: Opponent's view (after `reverse_perspective()`)
3. Value propagated with sign flip at each level

### Move Validation

- Policy outputs are **masked** to valid moves only
- Renormalized after masking
- Fallback to uniform distribution if all moves invalid

---

## ğŸ¯ Comparison to Other Agents

| Agent | Search | Evaluation | Trapdoors |
|-------|--------|-----------|-----------|
| **AlphaChicken** | MCTS + NN | Learned (DRL) | Bayesian |
| **Bob** | Alpha-Beta | Heuristic + Optional NN | Bayesian |
| **MinimaxAgent** | Minimax | Heuristic | None |
| **MCTSAgent** | MCTS | Random rollout | None |

---

## ğŸ› Debugging

### Model Not Loading

```
[AlphaChicken] Warning: best_model.pt not found
[AlphaChicken] Using randomly initialized network
```

**Solution**: Train the model first with `train_pace.py`

### Out of Time

If agent times out:
- Reduce `num_simulations` in `agent.py`
- Increase `time_safety_margin`
- Use faster hardware (GPU)

### Import Errors

Ensure `game` module is in parent directory:
```
3600-agents/
â”œâ”€â”€ AlphaChicken/
â”‚   â””â”€â”€ agent.py
â””â”€â”€ (game module imported from parent)
```

---

## ğŸ“š References

- **AlphaZero**: Silver et al. (2017) - "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm"
- **UCT**: Kocsis & SzepesvÃ¡ri (2006) - "Bandit based Monte-Carlo Planning"
- **ResNets**: He et al. (2015) - "Deep Residual Learning for Image Recognition"

---

## ğŸ”® Future Enhancements

1. **Data Augmentation**: Board rotations/reflections
2. **Opening Book**: Pre-computed strong openings
3. **Transfer Learning**: Bootstrap from heuristic agents
4. **Hyperparameter Tuning**: Grid search for optimal c_puct, lr, etc.
5. **Distributed Training**: Multi-GPU self-play
6. **Curriculum Learning**: Gradually increase opponent strength

---

## ğŸ‘¥ Author

CS3600 AlphaChicken Implementation

**Status**: Ready for training and competition ğŸš€


